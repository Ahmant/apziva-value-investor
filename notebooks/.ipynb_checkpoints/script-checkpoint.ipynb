{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQRTvAOuIh6d"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfcoMcFoIkL4",
    "tags": []
   },
   "source": [
    "## 1. Importing neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "\n",
    "!pip install tensorflow\n",
    "!pip install transformers\n",
    "!pip install sentence-transformers\n",
    "\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BhQXTijINtP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJuXDxUMIhbh",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-9zetYhdIyOd",
    "outputId": "4028d00c-5786-42e8-c0b3-1338e563791b"
   },
   "outputs": [],
   "source": [
    "dataset_original = pd.read_csv('./data/raw/data.csv')\n",
    "dataset_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZxTA2xYJxMd",
    "tags": []
   },
   "source": [
    "# Inspecting & Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWmbO7ciKAO_"
   },
   "outputs": [],
   "source": [
    "dataset_cleaned_temp = dataset_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPCcKoVxLS9Z",
    "outputId": "eed77581-73b0-4d5d-9fce-3372e4399d3d"
   },
   "outputs": [],
   "source": [
    "dataset_cleaned_temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned_temp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVstE0AtLLvL"
   },
   "source": [
    "### Checking missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned_temp.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned_temp.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary words & Replace abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned_temp['job_title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "spacy_nlp.pipe_names\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations_to_replace = {\n",
    "    'GPHR': 'Global Professional in Human Resources',\n",
    "    'CSR': 'Corporate Social Responsibility',\n",
    "    'MES': 'Manufacturing Execution Systems',\n",
    "    'SPHR': 'Senior Professional in Human Resources',\n",
    "    'SVP': 'Senior Vice President',\n",
    "    'GIS': 'Geographic Information System',\n",
    "    'RRP': 'Reduced Risk Products',\n",
    "    'CHRO': 'Chief Human Resources Officer',\n",
    "    'HRIS': 'Human resources information system',\n",
    "    'HR': 'Human resources',\n",
    "}\n",
    "\n",
    "def replace_abbreviations(sentence):\n",
    "    replaced_sentence = sentence\n",
    "    for abbreviation, replacement in abbreviations_to_replace.items():\n",
    "        # Create a regular expression pattern to match the whole word\n",
    "        pattern = r'\\b{}\\b'.format(re.escape(abbreviation))\n",
    "    \n",
    "        # Use re.sub() to replace the word in the sentence\n",
    "        replaced_sentence = re.sub(pattern, replacement, replaced_sentence, flags=re.IGNORECASE)\n",
    "\n",
    "    return replaced_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # Remove special characters\n",
    "    new_sentence = re.sub(r'[+*,.|(){}&\\-\\']', '', sentence)\n",
    "\n",
    "    # Replce abbreviations\n",
    "    new_sentence = replace_abbreviations(new_sentence)\n",
    "    \n",
    "    words = new_sentence.split()\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        stemmed_words.append(stemmer.stem(word))\n",
    "        \n",
    "    # Lemmatization\n",
    "    lemmatized_words = []\n",
    "    doc = spacy_nlp(\" \".join(stemmed_words))\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            lemmatized_words.append(token.lemma_)\n",
    "\n",
    "    return \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned_temp['job_title_cleaned'] = dataset_cleaned_temp['job_title'].apply(clean_sentence)\n",
    "print(dataset_cleaned_temp['job_title_cleaned'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([*set(dataset_cleaned_temp[\"job_title_cleaned\"].str.split().agg(sum, axis = 0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bR931S7Laca"
   },
   "outputs": [],
   "source": [
    "dataset_cleaned = dataset_cleaned_temp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UH5QdnRML6Fs",
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preprocessed = dataset_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup BERT & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(sentences):\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize input sentence\n",
    "        encoded_inputs = bert_tokenizer(sentence, padding=True, truncation=True, return_tensors='tf')\n",
    "    \n",
    "        # Generate BERT embeddings\n",
    "        outputs = bert_model(encoded_inputs)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Apply pooling strategy - averaging\n",
    "        pooled = tf.reduce_mean(hidden_states, axis=1)\n",
    "        embeddings.append(pooled.numpy().reshape(-1))\n",
    "    \n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def encode_and_get_similarity(data, queries, search_columns, output_columns):\n",
    "    data = data.copy()\n",
    "    \n",
    "    embeddings = {}\n",
    "    queries_embeddings = []\n",
    "    \n",
    "    # without replacing the abbreviations with their full meaning, we will get very bad results\n",
    "    for index, query in enumerate(queries):\n",
    "        query = replace_abbreviations(query)\n",
    "        query = clean_sentence(query)\n",
    "        queries_embeddings.append(get_bert_embeddings([query]))\n",
    "        \n",
    "    queries_embeddings_mean = np.mean(queries_embeddings, axis=0)\n",
    "    # queries_embeddings_mean = get_bert_embeddings('Aspiring Human Resources Professional')\n",
    "\n",
    "    for index, column in enumerate(search_columns):\n",
    "        sentences = dataset_preprocessed[column].tolist()\n",
    "\n",
    "        # Encoding\n",
    "        embeddings[column] = get_bert_embeddings(sentences)\n",
    "\n",
    "        # Cosine Similarity\n",
    "        cosine_similarities = cosine_similarity(\n",
    "            queries_embeddings_mean,\n",
    "            embeddings[column]\n",
    "        )        \n",
    "        data[output_columns[index]] = cosine_similarities[0]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Queries/Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    # 'Aspiring Human Resources Professional',\n",
    "    'aspiring human resources',\n",
    "    'seeking human resources'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embeddings & Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preprocessed = encode_and_get_similarity(dataset_preprocessed, queries, ['job_title_cleaned'], ['bert_similarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preprocessed.sort_values(by='bert_similarity', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starred Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark them as favorite/bookmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starred_ids = [int(item) for item in input(\"Enter the ids of the candidates you want to star (separate by spaces): \").split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Rank (Re-Rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similar to bookmark\n",
    "- First way:  Marging the keypharse and the starred title\n",
    "- Second way: one more column of scores (starred), use the starred job title as a keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preprocessed.loc[dataset_preprocessed['id'].isin(starred_ids), 'is_starred'] = 1\n",
    "dataset_preprocessed.loc[~dataset_preprocessed['id'].isin(starred_ids), 'is_starred'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_starred_score(data):\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Starred Queries\n",
    "    queries = data[data['is_starred'] == 1]['job_title_cleaned']\n",
    "    \n",
    "    similarities = []\n",
    "    for query in queries:\n",
    "        print('START: ' + query)\n",
    "        data = encode_and_get_similarity(data, [query], ['job_title_cleaned'], ['starred_similarity'])\n",
    "        similarities.append(data['starred_similarity'])\n",
    "        \n",
    "        \n",
    "    starred_similarity = np.mean(similarities, axis=0)\n",
    "    \n",
    "    return starred_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preprocessed['starred_similarity'] = get_starred_score(dataset_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preprocessed['mean_similarity'] = dataset_preprocessed[['bert_similarity', 'starred_similarity']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_preprocessed[['job_title', 'is_starred', 'bert_similarity', 'starred_similarity', 'mean_similarity']].sort_values(by=['mean_similarity', 'is_starred'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
